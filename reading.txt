Minimal reading plan (30–90 min)

20–40 min — the hands-on walkthrough you already linked (the Substack article): read start→finish. Focus: record layout, append-only log, in-memory index, compaction overview, tombstones. Outcome: you can paraphrase “how writes are stored and why reads are fast.”
15–30 min — the original Bitcask design paper (Basho): skim intro, data format / file layout sections, compaction and durability sections, and the conclusion. Outcome: you understand the original invariants and assumptions (single-writer, index-in-memory, crash/recovery approach).
Optional 15–30 min — short synthesis: read a 2–5 page summary or chapter extract on storage engines from “Designing Data-Intensive Applications” (or a short blog post about log-structured storage). Outcome: you get the broader context for tradeoffs (memory vs disk, fsync costs).
Concrete topics to absorb (quick flashcards — 5–10 min each)

Append-only log: why append-only simplifies concurrency and recovery.
Record format: header fields you’ll need (key-length, value-length, tombstone flag, checksum).
In-memory index: what it stores (segment id, offset, value length) and why it makes reads O(1).
Tombstones and deletes: how deletes are represented and why compaction is required.
Compaction: basic strategy (merge live keys into new segments) and atomic swap of files.
Durability model: fsync per write vs batched fsync vs background flush — tradeoffs.
Concurrency model: Bitcask’s single-writer/multi-reader assumption and file locking basics.
Recovery: how to rebuild index by scanning segments and handling truncated writes.